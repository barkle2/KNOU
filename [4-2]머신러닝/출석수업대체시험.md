# 1장 머신러닝 소개

## 1. 머신러닝의 개념

- 인공지능(강인공지능, 약인공지능) ⊃ 머신러닝 ⊃ 딥러닝
- 인공지능: 인간의 지능을 모방하여 문제해결을 위해 사람처럼 학습하고 이해하는 기계를 만드는 분야
- 머신러닝: 인간이 가지고 있는 고유의 지능적 기능 중 하나인 학습능력을 기계를 통해 구현하는 분야
  - 어떤 문제에 대해 명시적인 지식 표현이나 프로그램을 만드는 것이 어렵거나 불가능한 경우에 주로 사용함
- 딥러닝: 심층 신경망을 이용하여 데이터를 분석하는 학습에 초점을 둔 머신러닝 방법

## 2. 머신러닝의 처리 과정

- 학습 단계와 추론 단계로 구성
- 학습 단계: 주어진 데이터에 대한 분석을 통해 원하는 입·출력의 관계를 알려주는 매핑 함수(결정함수)를 찾는 과정
- 추론 단계: 학습을 통해 찾은 매핑 함수를 새롭게 주어지는 실제 데이터에 적용하여 결과를 얻는 과정

## 3. 머신러닝의 기본 요소

- 하나의 데이터는 n차원의 열벡터 **x** = [*x<sub>1</sub>, x<sub>2</sub>, ... , x<sub>n</sub>*]<sup>T</sup> 로 표현하며, 데이터 처리는 벡터 연산으로 정의됨
- 전체 데이터 집합이 이루는 분포 특성을 고려하여 특징을 추출하고 학습을 수행하는 것이 중요함
- 특징추출: 데이터에서 불필요한 정보를 제거하고 데이터 처리를 위한 핵심적 정보인 특징을 얻는 것
- 학습 시스템: 데이터로부터 학습을 통해 추출하고자 하는 정보를 표현하는 시스템
- 목적함수: 주어진 데이터 집합을 이용하여 학습 시스템이 달성해야 하는 목표를 기계가 알 수 있는 수학적 함수로 정의한 것
- 오차함수: 학습 시스템의 출력과 원하는 출력의 차이(오차)로 정의되는 목적함수
- 성능 평가 기준: 학습오차(학습 데이터 집합을 대상으로 계산된 오차), 테스트 오차(테스트 데이터 집합에 대한 오차), 일반화 오차(관찰될 수 있는 모든 데이터를 대상으로 하는 오차)
- 교차검증법: 제한된 데이터 집합을 이용하여 일반화 오차에 좀 더 근접한 오차값을 얻어 내는 방법

## 4. 머신러닝에서의 주제

- 머신러닝이 다루는 주제: 분류, 회귀, 군집화, 특징추출
- 분류: 입력 데이터가 어떤 부류(클래스)에 속하는지를 자동으로 판단하는 문제
  - 학습 데이터는 입력 데이터와 클래스 레이블의 쌍으로 구성됨
- 회귀: 학습을 통해 입력변수와 원하는 출력변수 사이의 매핑 관계를 분석하고 예측하는 것. 출력은 연속적인 실수값임
- 군집화: 주어지는 클래스 정보 없이 단순히 하나의 덩어리로 이루어진 데이터를 받아서, 데이터의 성질 또는 분포 특성을 분석하여 임의로 복수 개의 그룹으로 나누는 것
- 특징추출: 학습 시스템의 학습 결과로 특정 매핑 함수를 얻게 됨. 새로운 데이터 **x<sub>new</sub>** 는 매핑 함수 f를 통해 특징벡터 **z<sub>new</sub>** 로 변환되어 출력된다.

## 5. 학습 시스템 관련 개념

- 머신러닝의 유형: 지도학습, 비지도학습, 강화학습 등
- 지도학습(교사학습): 학습을 수행할 때 시스템이 출력해야 할 목표 출력값을 함께 제공하는 방식으로, 분류와 회귀 문제에 적합한 유형임
- 비지도학습(비교사학습): 학습할 때 목표 출력값에 대한 정보가 제공되지 않는 방식으로, 군집화 문제에 적합함
- 강화학습: 원하는 출력값을 모르거나 알 수 없는 경우 출력값에 대해 정확한 값의 형태로 교사 신호를 줄 수 없어서 출력값에 대한 교사 신호를 보상 형태로 주는 방식
- 과다적합: 학습 시스템이 학습 데이터에 대해서만 지나치게 적합한 형태로 결정경계를 형성하여 오히려 일반화 성능이 떨어지는 현상

# 4장 지도학습: 분류

## 분류 개념과 관련 용어

- 분류: 주어진 데이터 집합에 대해 이미 정의된 몇 개의 클래스(부류)로 입력을 구분하는 문제
- 분류기(classifier): 분류 문제를 다루는 학습 시스템
- 결정경계: 클래스로의 분류 기준

## 베이즈 분류기와 K-NN 분류기의 기본 개념/동작

- 베이즈 분류기
  - 후험확률에 대한 베이즈 정리로부터 유도된 판별함수 g<sub>i</sub>(**x**) = p(**x**|C<sub>i</sub>)p(C<sub>i</sub>) 를 이용하여 분류하는 방식
  - 새로운 데이터가 주어지면 각 클래스에 대해 g<sub>i</sub>(x)의 값을 계산한 후, 그 값이 가장 큰 클래스로 데이터를 분류함

- K-NN 분류기(K-Nearest Neighbor Classifier)
  - 주어진 데이터로부터 거리가 가까운 순서대로 K개의 데이터를 찾은 후, 그 중 가장 많은 수의 데이터가 속한 클래스로 할당하여 분류하는 방식
  - 분류 과정에서 새로운 데이터가 주어질 때마다 학습 데이터 전체와의 거리 계산을 통해 K개의 이웃 데이터를 선정해 주어야 하므로 항상 학습 데이터를 저장해야 함
    - 데이터의 수가 증가하면 그에 비례하여 계산량과 메모리도 함께 증가하는 문제점

## 이진 분류 문제에서의 베이즈 분류기에서의 결정경계의 형성

- 이진 분류 문제의 전체 데이터 집합ㅇ베서 각 클래스가 차지하는 비율이 p(C<sub>2</sub>)=αp(C<sub>1</sub>) 이라면 p(x|C<sub>2</sub>)=αp(x|C<sub>1</sub>) 를 만족하는 지점이 결정경계가 됨

## 가우시안 베이즈 분류기의 공분산행렬 형태에 따른 판별함수

- 클래스별 확률밀도함수가 가우시안 분포를 따르는 경우에는 공분산행령의 형태(클래스 공통 단위 공분산행렬, 클래스 공통 공분산행렬, 일반적인 공분산행렬)에 따라 결정경계와 판별함수가 달라짐
  - 클래스 공통 단위 공분산행렬: y(x) = argmin<sub>i</sub>{(x-μ<sub>i</sub>)<sup>T</sup>(x-μ<sub>i</sub>)}
  - 클래스 공통 공분산행렬: y(x) = argmin<sub>i</sub>{(x-μ<sub>i</sub>)<sup>T</sup>Σ<sup>-1</sup>(x-μ<sub>i</sub>)}
  - 일반적인 공분산행렬: y(x) = argmin<sub>i</sub>{(x-μ<sub>i</sub>)<sup>T</sup>Σ<sup>-1</sup>(x-μ<sub>i</sub>) + ln|Σ<sub>i</sub>|}

## K-NN 분류기 vs 가우시안 베이즈 분류기

- 베이즈 분류기는 데이터의 확률분포함수를 미리 가정하고 이를 추정하여 분류에 활용
  - 미리 가정한 확률 모델이 주어진 데이터 분포에 적합하지 않으면 좋은 성능을 기대하기 힘들다.
- 이러한 문제에 대한 대안으로 K-NN 분류기를 생각해볼 수 있다.
  - K-NN 분류기는 주어진 데이터로부터 거리가 가까운 순서대로 K개의 데이터를 찾은 후, 그중 가장 많은 수의 데이터가 속한 클래스로 할당하는 방법

## 거리 함수의 종류와 개념

- 2차 노름(유클리디안 거리): 공간상의 두 점을 잇는 직선거리의 길이
- 1차 노름: 각 좌표값의 차이를 모두 더한 값
- p차 노름: p의 값을 조정하여 거리 계산
- 내적: 두 벡터의 방향이 비슷할 수록 그 값이 커진다.(유사한 정도를 파악) 크기에도 영향을 받음
- 코사인 거리: 두 벡터 간의 각도 차이만으로 거리를 평가
- 정규화된 유클리디안 거리, 마할라노비스 거리: 각 좌표축 방향으로의 분산의 차이를 고려하여 반영해 준 거리 함수

## 분류기의 종류

- 베이즈 분류기, K-NN 분류기, 로지스틱 회귀, 결정 트리, SVM, 신경망 등

# 5장 회귀

## 회귀의 개념 및 적용 방법의 종류

- 회귀: 입력을 출력으로 매핑하는 함수를 찾는 문제
  - 입력값, 출력값은 실수
  - 학습 목표는 최적의 회귀함수 f(x:θ) 를 찾는 것
  - 응용 예: 시계열 예측 분야(주가 예측, 시장 예측, 공정 예측 등)
- 적용방법
  - 보간법: 제곱오차가 0, but 얻어지는 곡선이 매우 복잡
  - 회귀: 간단한 직선을 통해 입력과 출력의 관계를 설명, but 오차 발생

## 선형회귀의 개념과 목적

- 선형회귀(linear regression): 입력(독립변수)과 출력(종속변수) 쌍의 데이터를 통해 입력과 출력의 관계를 설명하는 선형 모델(일차식, 직선)을 찾는 문제
- 데이터 집합 D 가 주어졌을 때 (x,y) 관계를 설명할 수 있는 선형함수 y=w<sub>1</sub>x + w<sub>0</sub> + e 를 찾는 것

## 회귀함수를 사용한 새 데이터에 대한 예측

- 학습을 통해 회귀함수가 구해진 후, 새로운 입력 데이터 x<sub>new</sub>에 대한 예측 결과는 y=w<sub>1</sub>x + w<sub>0</sub> 를 통해 얻을 수 있다.

## 선형회귀의 비선형 문제를 위한 선형화의 개념

- x와 y가 선형 매핑의 관계로 표현할 수 없는 경우: 원래 데이터가 분포하는 형태에 따라 x와 y를 적절히 선형화하여 x'와 y'를 얻고, 이들 간의 선형 매핑관계 y'=mx'+b를 찾는 방식으로 진행

## 로지스틱 회귀의 개념 및 관련 용어/개념

- 로지스틱 회귀: 선형회귀분석의 출력을 범주형으로 제한한 회귀분석
  - 분류문제에 적용함
- 오즈비: 입력 x가 클래스 C1에 속할 확률과 C2에 속할 확률의 비율
- 로짓 함수: 오즈비에 대해 로그를 취한 함수

# 6장 군집화

## 군집화의 개념, 적용 방법 및 응용문제

- 군집화(clustering): 클래스에 대한 레이블을 가지지 않고 주어진 데이터를 분석하는 방법으로, 각 데이터의 유사도를 중심으로 몇 개의 그룹으로 나누는 것
- 군집화를 위한 데이터 집합의 경우 바람직한 출력값에 대한 정보가 없으므로 비지도학습을 수행해야 한다
- 군집화 응용문제: 영상, 이미지 분류 등

## K-평균 알고리즘의 수행 과정 및 특성

- K-평균 군집화 알고리즘: 데이터 집합을 K개의 그룹으로 묶는 알고리즘
- 수행과정
  - ① 임의로 K개의 벡터를 선택하여 K개의 초기 대표 벡터 집합을 생성
  - ③ 각 데이터에 대해 K개의 대표 벡터들과의 거리를 계산하고, 가장 가까운 벡터에 속하도록 레이블링
  - ③ 새로운 클러스터에서 각각의 대표 벡터를 갱신
  - ④ 반복여부 결정: 대표 벡터의 변화 or 설정된 반복 횟수 도달
- 특성: 실제 문제에 적용할 때 고려사항
  - 대표 벡터의 계산과 군집화 과정의 반복적인 수행을 통하여 좋은 군집을 찾는 것이 확실히 보장되는가?
    - 각 단계를 번갈아 반복할 때마다 목적함수의 값을 감소시켜서 지역 극소점을 찾음을 보장함
  - 초기화 과정에서 대표 벡터의 설정 방법이 군집화의 성은에 어떤 영향을 미치는가?
    - 초기에 임의로 결정한 초기값(대표 벡터)들을 잘 선택해 주는 것은 좋은 군집화 결과를 얻기 위해 매우 중요한 과정이며, 각 데이터 간의 거리가 어느 기준 이상 떨어진 것들을 후보로 선택하거나 전체 입력 범위를 균등 분할하여 선택하는 등의 방법을 사용함
  - 적절한 K값은 데이터에 의존하게 되는데, 어떻게 K값을 적절히 선택할 것인가?
    - 적절한 K값의 선정은 중요하지만, 이는 지극히 문제의존적인 특성을 가짐

## 계층적 군집화의 개념 및 각 방법의 개념

- 계층적 군집화 알고리즘: 전체 데이터를 몇 개의 배타적인 그룹으로 나누는 대신, 큰 군집이 작은 군집을 포함하는 형태로 계층을 이루도록 군집화를 수행하여 그 구조를 살펴보는 방법
  - 병합적 방법: 각 데이터가 하나의 군집을 이루는 최소 군집에서 시작하여 가까운 군집들끼리 단계적으로 병합하여 더 큰 군집을 만들어가는 방법
  - 분할적 방법: 모든 데이터가 하나의 군집에 속하는 최대 군집에서 시작하여 특정 기준에 따라 군집을 분할해 가는 방법

## 군집 간의 거리 계산 방식

- 최단 연결법: 가장 가까운 데이터 쌍 간의 거리
- 최장 연결법: 가장 멀리 떨어진 데이터 쌍 간의 거리
- 중심 연결법: 두 군집의 평균 간의 거리
- 평균 연결법: 모든 데이터 쌍 간 거리의 평균
- Ward's 방법: 병합 후의 클러스터 내부의 분산값

# 7장 특징추출

## 특징추출의 개념과 목적 등

- 특징추출: n차원의 입력 벡터 x에 대해 변환함수 Φ를 적용하여 m차원의 특징 벡터 y를 얻는 변환 y=Φ(x)
  - 목적: 불필요한 정보를 제거하고 핵심이 되는 정보만 추출 또는 데이터의 차원 축소를 통한 학습 시스템의 효율 향상

## 선형변환에 의한 차원축소의 개념와 예

- 선형변환에 의한 특징추출: Y=W<sup>T</sup>X
  - 주어진 데이터 X를 변환행렬 W에 의해 정해지는 방향으로 사영함으로써 저차원 특징값 Y를 얻는 것

## PCA, LDA의 목적, 개념 및 특성/문제점

- 주성분분석법(PCA): 변환 전의 데이터 X가 가지고 있는 정보를 차원 축소 후에도 최대한 유지하도록 하는 것
  - 변환행렬 W를 어떻게 찾는가?
    - 데이터 손실량을 최소로 하는 사영 벡터를 찾음
    - 데이터 집합의 분산이 가장 큰 방향을 찾음
    - 데이터의 공분산행렬의 고유치와 고유벡터를 찾아 고유치가 가장 큰 값부터 순서대로 m개에 대응하는 고유벡터로 변환행렬을 구성함
  - 특성
    - 데이터 분석에 대한 특별한 목적이 없는 경우에 유용한 방법
    - 클래스 레이블 정보를 활용하지 않는 비지도학습에 해당 -> 분류에 핵심이 되는 정보의 손실을 초래함
    - 데이터 분포가 비선형 구조를 가진 경우에는 이를 반영한 저차원 특징을 찾는 것이 불가능함
  
- 선형판별분석법(LDA): 클래스 레이블 정보를 적극 활용하여 클래스 간 판별이 용이한 방향으로 차원을 축소시키는 변환행렬을 찾음
  - 클래스 간의 거리는 가능한 멀어지게 하고, 같은 클래스 내에서는 결집되도록 하여 분류에 적합한 특징으로의 변환을 유도
  - 특성
    - PCA와 마찬가지로 데이터가 복잡한 비선형 구조를 가진 경우에는 적절한 변환을 수행하지 못함
    - 고유치 분석을 통해 찾아지는 고유벡터의 개수가 제한됨
    - 데이터의 수가 입력 차원보다 크지 않으면 클래스 내 산점행렬이 특이행렬이 되어 역행렬을 찾을 수 없어 고유치 분석을 수행할 수 없음

## MDS, t-SNE, Isomap의 개념

- 거리 기반 차원 축소 방법: 두 데이터 간의 거리(또는 유사도)를 핵심 정보로 사용하여 차원을 축소하는 방법
  - 종류: MDS, t-SNE, Isomap 등
  - 특징
    - 특정값을 얻기 위해 입력 데이터와 특징 데이터 간의 매핑 함수를 정의하지 않는다
    - 저차원의 특징값에 대한 목적함수를 정의하고 이를 최적화하는 특징값을 찾음
    - 현재 주어진 데이터를 잘 표현하는 특징을 찾지만, 새로운 데이터에 대해서는 특징값을 찾지 못함
    - 데이터 시각화의 용도로 주로 사용됨
- 다차원 척도법(MDS): 원래 데이터 쌍의 거리 d<sub>ij</sub> 와 추출된 특징 쌍의 유클리디안 거리 σ<sub>ij</sub>에 대한 분석을 통해 함수 Σ(d<sub>ij</sub>-σ<sub>ij</sub>)<sup>2</sup> 를 최소화하는 특징을 찾는 방법
  - 특징값은 원래 데이터가 가지는 거리 관계를 유클리디안 좌표 명면상에서의 거리로 표현 가능
- SNE(통계적 이웃 임베딩, Stochastic Neighbor Embedding): 가우시안 분포를 따른다고 가정하고 유사도를 조건부확률을 이용하여 정의
  - 특정값은 원래 데이터가 가지는 확률적 유사도를 잘 표현함
  - t-SNE 기법: 추출된 특징 쌍의 유사도를 정의할 때 t-분포를 사용하여 거리가 멀리 떨어진 데이터 사이의 관계를 더 잘 반영함
- Isomap: 기하학적 다양체(매니폴드) 상에서의 측지 거리를 사용하는 차원 축소 방법
  - 데이터들을 정점으로 가지는 그래프 간의 경로를 다익스트라 알고리즘으로 계산 가능
  
