# 1장 머신러닝 소개

## 1. 머신러닝의 개념

- 인공지능(강인공지능, 약인공지능) ⊃ 머신러닝 ⊃ 딥러닝
- 인공지능: 인간의 지능을 모방하여 문제해결을 위해 사람처럼 학습하고 이해하는 기계를 만드는 분야
- 머신러닝: 인간이 가지고 있는 고유의 지능적 기능 중 하나인 학습능력을 기계를 통해 구현하는 분야
  - 어떤 문제에 대해 명시적인 지식 표현이나 프로그램을 만드는 것이 어렵거나 불가능한 경우에 주로 사용함
- 딥러닝: 심층 신경망을 이용하여 데이터를 분석하는 학습에 초점을 둔 머신러닝 방법

## 2. 머신러닝의 처리 과정

- 학습 단계와 추론 단계로 구성
- 학습 단계: 주어진 데이터에 대한 분석을 통해 원하는 입·출력의 관계를 알려주는 매핑 함수(결정함수)를 찾는 과정
- 추론 단계: 학습을 통해 찾은 매핑 함수를 새롭게 주어지는 실제 데이터에 적용하여 결과를 얻는 과정

## 3. 머신러닝의 기본 요소

- 하나의 데이터는 n차원의 열벡터 **x** = [*x<sub>1</sub>, x<sub>2</sub>, ... , x<sub>n</sub>*]<sup>T</sup> 로 표현하며, 데이터 처리는 벡터 연산으로 정의됨
- 전체 데이터 집합이 이루는 분포 특성을 고려하여 특징을 추출하고 학습을 수행하는 것이 중요함
- 특징추출: 데이터에서 불필요한 정보를 제거하고 데이터 처리를 위한 핵심적 정보인 특징을 얻는 것
- 학습 시스템: 데이터로부터 학습을 통해 추출하고자 하는 정보를 표현하는 시스템
- 목적함수: 주어진 데이터 집합을 이용하여 학습 시스템이 달성해야 하는 목표를 기계가 알 수 있는 수학적 함수로 정의한 것
- 오차함수: 학습 시스템의 출력과 원하는 출력의 차이(오차)로 정의되는 목적함수
- 성능 평가 기준: 학습오차(학습 데이터 집합을 대상으로 계산된 오차), 테스트 오차(테스트 데이터 집합에 대한 오차), 일반화 오차(관찰될 수 있는 모든 데이터를 대상으로 하는 오차)
- 교차검증법: 제한된 데이터 집합을 이용하여 일반화 오차에 좀 더 근접한 오차값을 얻어 내는 방법

## 4. 머신러닝에서의 주제

- 머신러닝이 다루는 주제: 분류, 회귀, 군집화, 특징추출
- 분류: 입력 데이터가 어떤 부류(클래스)에 속하는지를 자동으로 판단하는 문제
  - 학습 데이터는 입력 데이터와 클래스 레이블의 쌍으로 구성됨
- 회귀: 학습을 통해 입력변수와 원하는 출력변수 사이의 매핑 관계를 분석하고 예측하는 것. 출력은 연속적인 실수값임
- 군집화: 주어지는 클래스 정보 없이 단순히 하나의 덩어리로 이루어진 데이터를 받아서, 데이터의 성질 또는 분포 특성을 분석하여 임의로 복수 개의 그룹으로 나누는 것
- 특징추출: 학습 시스템의 학습 결과로 특정 매핑 함수를 얻게 됨. 새로운 데이터 **x<sub>new</sub>** 는 매핑 함수 f를 통해 특징벡터 **z<sub>new</sub>** 로 변환되어 출력된다.

## 5. 학습 시스템 관련 개념

- 머신러닝의 유형: 지도학습, 비지도학습, 강화학습 등
- 지도학습(교사학습): 학습을 수행할 때 시스템이 출력해야 할 목표 출력값을 함께 제공하는 방식으로, 분류와 회귀 문제에 적합한 유형임
- 비지도학습(비교사학습): 학습할 때 목표 출력값에 대한 정보가 제공되지 않는 방식으로, 군집화 문제에 적합함
- 강화학습: 원하는 출력값을 모르거나 알 수 없는 경우 출력값에 대해 정확한 값의 형태로 교사 신호를 줄 수 없어서 출력값에 대한 교사 신호를 보상 형태로 주는 방식
- 과다적합: 학습 시스템이 학습 데이터에 대해서만 지나치게 적합한 형태로 결정경계를 형성하여 오히려 일반화 성능이 떨어지는 현상

# 4장 지도학습: 분류

## 분류 개념과 관련 용어

- 분류: 주어진 데이터 집합에 대해 이미 정의된 몇 개의 클래스(부류)로 입력을 구분하는 문제
- 분류기(classifier): 분류 문제를 다루는 학습 시스템
- 결정경계: 클래스로의 분류 기준

## 베이즈 분류기와 K-NN 분류기의 기본 개념/동작

- 베이즈 분류기
  - 후험확률에 대한 베이즈 정리로부터 유도된 판별함수 g<sub>i</sub>(**x**) = p(**x**|C<sub>i</sub>)p(C<sub>i</sub>) 를 이용하여 분류하는 방식
  - 새로운 데이터가 주어지면 각 클래스에 대해 g<sub>i</sub>(x)의 값을 계산한 후, 그 값이 가장 큰 클래스로 데이터를 분류함

- K-NN 분류기(K-Nearest Neighbor Classifier)
  - 주어진 데이터로부터 거리가 가까운 순서대로 K개의 데이터를 찾은 후, 그 중 가장 많은 수의 데이터가 속한 클래스로 할당하여 분류하는 방식
  - 분류 과정에서 새로운 데이터가 주어질 때마다 학습 데이터 전체와의 거리 계산을 통해 K개의 이웃 데이터를 선정해 주어야 하므로 항상 학습 데이터를 저장해야 함
    - 데이터의 수가 증가하면 그에 비례하여 계산량과 메모리도 함께 증가하는 문제점

## 이진 분류 문제에서의 베이즈 분류기에서의 결정경계의 형성

- 이진 분류 문제의 전체 데이터 집합ㅇ베서 각 클래스가 차지하는 비율이 p(C<sub>2</sub>)=αp(C<sub>1</sub>) 이라면 p(x|C<sub>2</sub>)=αp(x|C<sub>1</sub>) 를 만족하는 지점이 결정경계가 됨

## 가우시안 베이즈 분류기의 공분산행렬 형태에 따른 판별함수

- 클래스별 확률밀도함수가 가우시안 분포를 따르는 경우에는 공분산행령의 형태(클래스 공통 단위 공분산행렬, 클래스 공통 공분산행렬, 일반적인 공분산행렬)에 따라 결정경계와 판별함수가 달라짐
  - 클래스 공통 단위 공분산행렬: y(x) = argmin<sub>i</sub>{(x-μ<sub>i</sub>)<sup>T</sup>(x-μ<sub>i</sub>)}
  - 클래스 공통 공분산행렬: y(x) = argmin<sub>i</sub>{(x-μ<sub>i</sub>)<sup>T</sup>Σ<sup>-1</sup>(x-μ<sub>i</sub>)}
  - 일반적인 공분산행렬: y(x) = argmin<sub>i</sub>{(x-μ<sub>i</sub>)<sup>T</sup>Σ<sup>-1</sup>(x-μ<sub>i</sub>) + ln|Σ<sub>i</sub>|}

## K-NN 분류기 vs 가우시안 베이즈 분류기

- 베이즈 분류기는 데이터의 확률분포함수를 미리 가정하고 이를 추정하여 분류에 활용
  - 미리 가정한 확률 모델이 주어진 데이터 분포에 적합하지 않으면 좋은 성능을 기대하기 힘들다.
- 이러한 문제에 대한 대안으로 K-NN 분류기를 생각해볼 수 있다.
  - K-NN 분류기는 주어진 데이터로부터 거리가 가까운 순서대로 K개의 데이터를 찾은 후, 그중 가장 많은 수의 데이터가 속한 클래스로 할당하는 방법

## 거리 함수의 종류와 개념

- 2차 노름(유클리디안 거리): 공간상의 두 점을 잇는 직선거리의 길이
- 1차 노름: 각 좌표값의 차이를 모두 더한 값
- p차 노름: p의 값을 조정하여 거리 계산
- 내적: 두 벡터의 방향이 비슷할 수록 그 값이 커진다.(유사한 정도를 파악) 크기에도 영향을 받음
- 코사인 거리: 두 벡터 간의 각도 차이만으로 거리를 평가
- 정규화된 유클리디안 거리, 마할라노비스 거리: 각 좌표축 방향으로의 분산의 차이를 고려하여 반영해 준 거리 함수

## 분류기의 종류

- 베이즈 분류기, K-NN 분류기, 로지스틱 회귀, 결정 트리, SVM, 신경망 등

# 5장 회귀

## 회귀의 개념 및 적용 방법의 종류

- 회귀: 입력을 출력으로 매핑하는 함수를 찾는 문제
  - 입력값, 출력값은 실수
  - 학습 목표는 최적의 회귀함수 f(x:θ) 를 찾는 것
  - 응용 예: 시계열 예측 분야(주가 예측, 시장 예측, 공정 예측 등)
- 적용방법
  - 보간법: 제곱오차가 0, but 얻어지는 곡선이 매우 복잡
  - 회귀: 간단한 직선을 통해 입력과 출력의 관계를 설명, but 오차 발생

## 선형회귀의 개념과 목적

- 선형회귀(linear regression): 입력(독립변수)과 출력(종속변수) 쌍의 데이터를 통해 입력과 출력의 관계를 설명하는 선형 모델(일차식, 직선)을 찾는 문제
- 데이터 집합 D 가 주어졌을 때 (x,y) 관계를 설명할 수 있는 선형함수 y=w<sub>1</sub>x + w<sub>0</sub> + e 를 찾는 것

## 회귀함수를 사용한 새 데이터에 대한 예측

- 학습을 통해 회귀함수가 구해진 후, 새로운 입력 데이터 x<sub>new</sub>에 대한 예측 결과는 y=w<sub>1</sub>x + w<sub>0</sub> 를 통해 얻을 수 있다.

## 선형회귀의 비선형 문제를 위한 선형화의 개념

- x와 y가 선형 매핑의 관계로 표현할 수 없는 경우: 원래 데이터가 분포하는 형태에 따라 x와 y를 적절히 선형화하여 x'와 y'를 얻고, 이들 간의 선형 매핑관계 y'=mx'+b를 찾는 방식으로 진행

## 로지스틱 회귀의 개념 및 관련 용어/개념

- 로지스틱 회귀: 선형회귀분석의 출력을 범주형으로 제한한 회귀분석
  - 분류문제에 적용함
- 오즈비: 입력 x가 클래스 C1에 속할 확률과 C2에 속할 확률의 비율
- 로짓 함수: 오즈비에 대해 로그를 취한 함수

# 6장 군집화

## 군집화의 개념, 적용 방법 및 응용문제

- 군집화(clustering): 클래스에 대한 레이블을 가지지 않고 주어진 데이터를 분석하는 방법으로, 각 데이터의 유사도를 중심으로 몇 개의 그룹으로 나누는 것
- 군집화를 위한 데이터 집합의 경우 바람직한 출력값에 대한 정보가 없으므로 비지도학습을 수행해야 한다
- 군집화 응용문제: 영상, 이미지 분류 등

## K-평균 알고리즘의 수행 과정 및 특성

- K-평균 군집화 알고리즘: 데이터 집합을 K개의 그룹으로 묶는 알고리즘
- 수행과정
  - ① 임의로 K개의 벡터를 선택하여 K개의 초기 대표 벡터 집합을 생성
  - ③ 각 데이터에 대해 K개의 대표 벡터들과의 거리를 계산하고, 가장 가까운 벡터에 속하도록 레이블링
  - ③ 새로운 클러스터에서 각각의 대표 벡터를 갱신
  - ④ 반복여부 결정: 대표 벡터의 변화 or 설정된 반복 횟수 도달
- 특성: 실제 문제에 적용할 때 고려사항
  - 대표 벡터의 계산과 군집화 과정의 반복적인 수행을 통하여 좋은 군집을 찾는 것이 확실히 보장되는가?
    - 각 단계를 번갈아 반복할 때마다 목적함수의 값을 감소시켜서 지역 극소점을 찾음을 보장함
  - 초기화 과정에서 대표 벡터의 설정 방법이 군집화의 성은에 어떤 영향을 미치는가?
    - 초기에 임의로 결정한 초기값(대표 벡터)들을 잘 선택해 주는 것은 좋은 군집화 결과를 얻기 위해 매우 중요한 과정이며, 각 데이터 간의 거리가 어느 기준 이상 떨어진 것들을 후보로 선택하거나 전체 입력 범위를 균등 분할하여 선택하는 등의 방법을 사용함
  - 적절한 K값은 데이터에 의존하게 되는데, 어떻게 K값을 적절히 선택할 것인가?
    - 적절한 K값의 선정은 중요하지만, 이는 지극히 문제의존적인 특성을 가짐

## 계층적 군집화의 개념 및 각 방법의 개념

- 계층적 군집화 알고리즘: 전체 데이터를 몇 개의 배타적인 그룹으로 나누는 대신, 큰 군집이 작은 군집을 포함하는 형태로 계층을 이루도록 군집화를 수행하여 그 구조를 살펴보는 방법
  - 병합적 방법: 각 데이터가 하나의 군집을 이루는 최소 군집에서 시작하여 가까운 군집들끼리 단계적으로 병합하여 더 큰 군집을 만들어가는 방법
  - 분할적 방법: 모든 데이터가 하나의 군집에 속하는 최대 군집에서 시작하여 특정 기준에 따라 군집을 분할해 가는 방법

## 군집 간의 거리 계산 방식

- 최단 연결법: 가장 가까운 데이터 쌍 간의 거리
- 최장 연결법: 가장 멀리 떨어진 데이터 쌍 간의 거리
- 중심 연결법: 두 군집의 평균 간의 거리
- 평균 연결법: 모든 데이터 쌍 간 거리의 평균
- Ward's 방법: 병합 후의 클러스터 내부의 분산값

# 7장 특징추출

## 특징추출의 개념과 목적 등

- 특징추출: n차원의 입력 벡터 x에 대해 변환함수 Φ를 적용하여 m차원의 특징 벡터 y를 얻는 변환 y=Φ(x)
  - 목적: 불필요한 정보를 제거하고 핵심이 되는 정보만 추출 또는 데이터의 차원 축소를 통한 학습 시스템의 효율 향상

## 선형변환에 의한 차원축소의 개념와 예

- 선형변환에 의한 특징추출: Y=W<sup>T</sup>X
  - 주어진 데이터 X를 변환행렬 W에 의해 정해지는 방향으로 사영함으로써 저차원 특징값 Y를 얻는 것

## PCA, LDA의 목적, 개념 및 특성/문제점

- 주성분분석법(PCA): 변환 전의 데이터 X가 가지고 있는 정보를 차원 축소 후에도 최대한 유지하도록 하는 것
  - 변환행렬 W를 어떻게 찾는가?
    - 데이터 손실량을 최소로 하는 사영 벡터를 찾음
    - 데이터 집합의 분산이 가장 큰 방향을 찾음
    - 데이터의 공분산행렬의 고유치와 고유벡터를 찾아 고유치가 가장 큰 값부터 순서대로 m개에 대응하는 고유벡터로 변환행렬을 구성함
  - 특성
    - 데이터 분석에 대한 특별한 목적이 없는 경우에 유용한 방법
    - 클래스 레이블 정보를 활용하지 않는 비지도학습에 해당 -> 분류에 핵심이 되는 정보의 손실을 초래함
    - 데이터 분포가 비선형 구조를 가진 경우에는 이를 반영한 저차원 특징을 찾는 것이 불가능함
  
- 선형판별분석법(LDA): 클래스 레이블 정보를 적극 활용하여 클래스 간 판별이 용이한 방향으로 차원을 축소시키는 변환행렬을 찾음
  - 클래스 간의 거리는 가능한 멀어지게 하고, 같은 클래스 내에서는 결집되도록 하여 분류에 적합한 특징으로의 변환을 유도
  - 특성
    - PCA와 마찬가지로 데이터가 복잡한 비선형 구조를 가진 경우에는 적절한 변환을 수행하지 못함
    - 고유치 분석을 통해 찾아지는 고유벡터의 개수가 제한됨
    - 데이터의 수가 입력 차원보다 크지 않으면 클래스 내 산점행렬이 특이행렬이 되어 역행렬을 찾을 수 없어 고유치 분석을 수행할 수 없음

## MDS, t-SNE, Isomap의 개념

- 거리 기반 차원 축소 방법: 두 데이터 간의 거리(또는 유사도)를 핵심 정보로 사용하여 차원을 축소하는 방법
  - 종류: MDS, t-SNE, Isomap 등
  - 특징
    - 특정값을 얻기 위해 입력 데이터와 특징 데이터 간의 매핑 함수를 정의하지 않는다
    - 저차원의 특징값에 대한 목적함수를 정의하고 이를 최적화하는 특징값을 찾음
    - 현재 주어진 데이터를 잘 표현하는 특징을 찾지만, 새로운 데이터에 대해서는 특징값을 찾지 못함
    - 데이터 시각화의 용도로 주로 사용됨
- 다차원 척도법(MDS): 원래 데이터 쌍의 거리 d<sub>ij</sub> 와 추출된 특징 쌍의 유클리디안 거리 σ<sub>ij</sub>에 대한 분석을 통해 함수 Σ(d<sub>ij</sub>-σ<sub>ij</sub>)<sup>2</sup> 를 최소화하는 특징을 찾는 방법
  - 특징값은 원래 데이터가 가지는 거리 관계를 유클리디안 좌표 명면상에서의 거리로 표현 가능
- SNE(통계적 이웃 임베딩, Stochastic Neighbor Embedding): 가우시안 분포를 따른다고 가정하고 유사도를 조건부확률을 이용하여 정의
  - 특정값은 원래 데이터가 가지는 확률적 유사도를 잘 표현함
  - t-SNE 기법: 추출된 특징 쌍의 유사도를 정의할 때 t-분포를 사용하여 거리가 멀리 떨어진 데이터 사이의 관계를 더 잘 반영함
- Isomap: 기하학적 다양체(매니폴드) 상에서의 측지 거리를 사용하는 차원 축소 방법
  - 데이터들을 정점으로 가지는 그래프 간의 경로를 다익스트라 알고리즘으로 계산 가능
  
# 8장 앙상블 학습

## 앙상블 학습의 개요

- 선형 분류기와 같은 간단한 학습기로 학습을 수행하되, 복수 개의 학습기를 결합함으로써 결과적으로 더 좋은 성능을 가진 학습기를 만들고자 하는 방법

## 배깅의 개념

- 배깅: 리샘플링 기법을 적용한 학습기 선택 방법
  - 주어진 제한된 크기의 학습 데이터 집합 X로부터 i번째 학습기를 위한 데이터를 랜덤하게 복원추출하여 데이터 집합 X<sub>i</sub>를 만들고, 이를 통해 학습을 수행
  - 이 과정을 M번 반복하여 서로 다른 M개의 학습기 생성
- 배깅에 의한 학습 시 고려사항: 전체 학습 데이터의 집합 X의 크기가 충분히 크지 않으면 X<sub>i</sub>의 크기를 X의 크기와 동일하게 설정하고, 학습기는 데이터 집합의 변화에 민감한 다층 퍼셉트론이나 최근접이웃 분류기 등이 바람직함

## 부스팅의 개념

- 부스팅: 간단한 학습기들이 상호보완적 역할을 할 수 있도록 단계적으로 학습을 수행하여 결합함으로써 그 성능을 증폭시키려는 방법
  - 학습기들이 순차적으로 학습하도록 하여 먼저 학습된 학습기의 결과가 다음 학습기의 학습에 정보를 제공함으로써, 이전의 학습기의 결점을 보완하는 방향으로 학습을 진행함
- 필터링에 의한 부스팅: 학습기별로 서로 다른 데이터 집합을 사용함
  - 전체 학습에 필요한 데이터의 규모가 매우 커야 한다는 문제점을 가짐
  - 이를 해결하기 위해 같은 데이터 집합을 반복해서 사용하면서 각 데이터의 가중치 조정을 통해 학습에 변화를 주는 AdaBoost가 개발됨

## AdaBoost 개념과 특징

- AdaBoost 알고리즘: 이전 단계의 분류기의 학습 결과를 활용하여 다음 단계의 학습에 사용될 데이터에 가중치를 부여함으로써 분류기 간의 차별성을 부여하고, 최종 결합단계에서는 학습에 사용된 가중치를 이용한 보팅 방법을 적용하여 작은 오분류율을 가진 분류기가 판단에 더 중요한 역할을 하도록 함
- 이진 분류 문제에 적합한 방법

# 9장 

## 결정 트리의 개념과 특징

- 결정 트리: 주어진문제에 관해 결정을 내리는 함수를 트리 형태로 구성한 것
  - 결과에 대한 뛰어난 설명력을 제공함
  - 적용 대상이 분류 문제에서 회귀 문제로 확장되어 CART 라고도 부름
- 루트 노드와 내부 노드는 결정요인(속성)에 해당하며, 결정 요인의 값에 따라 가지의 개수가 결정되고, 리프 노드는 최종 결과에 대응됨
  - 결정 트리는 학습을 통해 자동으로 생성됨
- 학습 과정: 루트 노드부터 시작하여 각 노드에 적절한 속성을 선택하고, 그 속성값에 따라 기준을 정하여 가지로 나누고 자식 노드를 추가함
  - 각 자식 노드를 다시 루트 노드로 두고 이 과정을 반복함
  - 어떤 자식 노드에 할당된 데이터의 클래스 레이블이 모두 동일하면 해당 노드는 더 이상의 분할이 필요 없는 리프 노드가 됨

## 지니 불순도와 지니 평가지수의 계산

- 지니 불순도: 각 노드에 할당된 데이터들의 혼합 정도를 측정하는 값
  - 하나의 그룹 안에 서로 다른 클래스 레이블들이 혼재할수록 불순도는 높아짐
- 지니 평가지수: 특정 노드에서 자식 노드의 지니 불순도의 가중합
  - 지니 평가지수를 최소화하는 속성 노드를 선택하여 레벨을 확장하는 과정을 반복함
- 회귀를 위한 결정 트리: 리프 노드가 가지는 출력값이 실수값이 되어야 하고, 이를 위해 해당 노드에 할당된 데이터들이 가지는 목표 출력값의 평균값을 사용함

## 결정 트리에 따른 결정 영역의 표현/형성

## 랜덤 포레스트의 개념

- 결정 트리의 문제: 학습 데이터가 가지는 노이즈까지 완벽하게 표현할 수 있을 정도로 과다적합되어 일반화 성능이 저하됨
  - 이를 위해 결정 트리와 앙상블 학습 기법을 결합한 랜덤 포레스트가 등장함
- 랜덤 포레스트: 배깅 방법으로 데이터를 리샘플링하여 복수 개의 결정 트리를 학습하고 결합하는 방법
- 학습과정: N개의 샘플로 이루어진 전체 데이터 집합 D로부터 중복을 허락하여 ~N 개의 샘플을 무작위로 추출해 결정 트리 학습을 위한 데이터 집합 D<sub>i</sub>을 생성하고, 이를 이용해 결정 트리를 학습하여 결정경계를 얻음. 이러한 과정을 반복하여 M개의 결정 트리를 만들고, 이들을 결합하여 최종 출력을 생성함
- 결합 방법: 분류 문제는 주로 보팅 기법, 회귀 문제는 출력값의 평균을 취하는 방법을 주로 사용함
- 사용한 결정 트리의 수가 증가함에 따라 정교한 결정경계가 만들어지고, 노이즈를 포함하는 데이터에 대하여 과다적합되는 현상이 많이 완화됨
- 생성 과정에서 많은 시간이 걸리고, 예측 결과를 해석하기 어려운 단점도 있음

# 10장 SVM과 커널법

## SVM(최대 마진 분류기)의 개념 및 마진/서포트벡터 개념

- 서포트 벡터: 결정경계에 가장 가까운 곳에 있는 데이터
- 마진: 학습 데이터 중에서 결정경계에 가장 가까운 데이터로부터 결정경계까지의 거리
- 서포트 벡터 머신(SVM)(또는 최대 마진 분류기): 일반화 오차를 작게 하기 위해서는 두 클래스 간의 간격을 최대로 하는 것이 좋으므로 마진을 최대로 하는 결정 경계를 찾는 것이 바람직하며, 이러한 목적에 맞춰 최적화된 선형 결정경계를 찾는 분류기

## SVM 학습의 결과와 분류 방법

## SVM에 적용되는 슬랙변수와 커널법의 목적과 개념

- 슬랙 변수: 잘못 분류된 데이터로부터 해당 클래스의 경계까지의 거리
  - 데이터가 해당 클래스의 결정경계를 넘어서 다른 클래스 영역에 존재할 수 있도록 허용함
  - 슬랙변수가 클수록 더 심한 오분류를 허용함을 의미함
  - α<sub>i</sub>가 c보다 작거나 같다는 조건만이 추가된 후, ~α<sub>i</sub> 값만 정해지면 ~ω와 ~ω<sub>i</sub>의 값은 슬랙변수가 없는 경우와 완전히 일치하므로, 분류 시에도 동일한 분류함수에 의에 분류함
- 커널법: 고차원 매핑을 통해 비선형 문제를 선형화하여 해결하면서 커널 함수를 통해 계산량 증가의 문제를 해결하는 방법
- 커널 함수: n차원의 입력 데이터 x를 m차원의 특징 데이터 Φ(x)로 매핑시킨 후 이를 SVM을 이용하여 분류하는 경우, 고차원 매핑 Φ(x)와 Φ(y)를 직접적으로 정의하는 대신에 두 벡터의 내적 Φ(x)·Φ(y)를 하나의 함수 k(x,y)로 정의하여 사용하며, 이렇게 정의되는 함수
- SVM에서 주로 사용되는 커널: 선형 커널, 다항식 커널, 시그모이드 커널, 가우시안 터널 등
  - 각 커널 함수는 고유의 파라미터를 가지고 있으며, 문제의 성격에 맞추어 적절히 조정해주어야 함

# 11장 신경망

## 생물학적 신경망

- 신경세포는 기능에 따라 수상돌기(입력), 세포체(연산), 축색돌기(출력)으로 나눌 수 있음
- 세포체 내부에서는 기본적으로 들어온 입력의 합이 일정 수준에 달하면 활동 전위를 발생시킴
- 두 신경세포는 시냅스를 통하여 연결되고, 연경 강도에 따라 정보의 전달 방식과 크기가 달라짐
- 신경세포들은 계층 구조와 같은 체계적인 연결 구조로 되어 있음

## 신경망의 구성 요소

- 신경망을 정의하는 세 가지 요소: 인공 신경세포(뉴런, 노드), 연결 구조, 학습 규칙
- 신경세포: n개의 입력에 대하여 연결 강도에 해당하는 가중치를 곱하여 모두 합한 가중합을 계산하고, 이에 대해 활성화 함수를 통하여 다음 뉴런으로 전달될 출력이 결정됨
- 활성화 함수: 하나의 뉴런의 특성을 결정하는 요소
  - 종류: 계단함수, 부호함수, 선형함수, 시그모이드 함수, 하이퍼탄젠트 함수, ReLU 함수 등
- 연결 구조: 층상 주고이며, 입력층 - [은닉층] - 출력층으로 구성됨
  - 은닉층의 유무에 따라 단층 신경망과 다층 신경망, 정보의 흐름에 따라 전방향 신경망과 회귀 신경망으로 구분함
- 학습: 신경망이 원하는 기능을 수행할 수 있도록 시냅스의 연결 강도(가중치)를 조정하는 것
  - 어떤 입력이 주어졌을 때 최종적으로 내는 출력이 원하는 값이 되도록 반복적인 가중치 수정을 통해 점점 원하는 기능에 근접해 가는 것
  - 오류 역전파 학습 알고리즘

## 신경망의 특성

- 활용 관점에서의 신경망: 입력 x를 받아 출력 y를 계산하는 함수 y=f(x) 임
  - 주어진 문제를 신경망으로 해결한다는 것은 주어진 데이터를 사용해서 신경망을 학습하는 과정을 의미함
  - 학습은 함수 f의 형태를 결정짓는 파라미터를 찾는 과정임
  - 함수로서 신경망의 장점: 표현 능력, 학습 능력, 일반화 능력

## 다층 퍼셉트론의 개념 및 구성, 표현 능력

- 퍼셉트론: M-P 뉴런을 여러 개 결합하여 네트워크 형태를 갖춘 신경망
  - 1958년 패턴인식 수행, 단층 전방향 신경망, 계단함수, 이진 입·출력, 지도학습
- 퍼셉트론의 한계: 퍼셉트론이 만드는 결정경계는 2차원 공간상 하나의 직선으로 나타나므로, XOR와 같은 출력을 내도록 결정경계를 만드는 것이 불가능함
  - 비선형 결정경계를 만들기 위하여 은닉층을 추가한 모델이 바로 다층 퍼셉트론임
- 다층 퍼셉트론: 1개 또는 그 이상의 은닉층을 가지는 다층 전방향 신경망
  - 은닉 뉴런의 활성화 함수로는 시그모이드나 하이퍼탄젠트와 같은 비선형 함수를 사용함
- 이론적으로는 하나의 은닉층을 가진 다층 퍼셉트론으로 어떠한 연속 함수도 원하는 오차만큼 가깝게 근사할 수 있으므로, 이를 이용하면 복잡한 비선형 결정경계를 가진 분류 문제에도 성공적으로 적용할 수 있음

## MLP 학습의 고려사항 및 학습 전략

- 다층 퍼셉트론의 학습: 지도학습
  - 오류 역전파 학습 알고리즘: 기울기 강하 학습법을 다층 퍼셉트론에 적용하여 실제로 구현 가능한 알고리즘화한 것
- 오류 역전파 학습 알고리즘: 학습 데이터의 출력값과 목표 출력값을 비교하여 출력 뉴런으로의 가중ㅈ치 수정항을 계싼하고, 다시 출력 뉴런의 오차를 이용하여 각 은닉 뉴런으로서의 가중치 수정할을 계산하여 가중치를 조정함. 이처럼 출력 뉴런의 오차가 은닉 뉴런에 거꾸로 전파되어 오는 형태를 가지므로 오류 역전파라고 부름
- 다층 퍼셉트론 학습의 고려사항: 지역 극소의 문제, 수렴 속도의 문제, 학습 종료점의 문제, 은닉 뉴런의 수 등
- 다층 퍼셉트론의 학습 전략
  - 학습 모드의 설정: 온라인 모드, 배치 모드, 미니 배치 모드
  - 모델 설정: 은닉 노드의 수, 초기 가중치 설정, 사용하는 활성화 함수의 종류 등
  - 오차함수: 제곱 오차함수, 교차엔트로피 오차함수 등

# 12장 딥러닝

## 신경망 학습의 문제점 및 학습 성능 향상을 위한 기법

### 딥러닝의 등장

- 심층 신경망: 많은 수의 은닉층을 가진 신경망(vs 얕은 신경망)
  - 은닉층의 개수를 늘려서 가중치의 개수가 늘어나면 표현 효율이 향상되지만 학습이 느려지고 낮은 일반화 성능 등의 어려움이 존재함
- 학습의 어려움을 극복하게 만든 요인: 충분히 큰 학습 데이터 집합, 컴퓨팅 파워 향상 및 GPU 기술 활용, 다양한 학습 기법들의 개발, 더 정교한 모델의 등장 등

### 학습 성능 향상을 위한 기법

- 지역 극소 문제: 기울기 강하 학습법의 근본 문제로, 학습이 실패하는 경우
  - 대안: 시뮬레이티드 어닐링 기법, 확률적 기울기 강하 학습법
- 느린 학습 문제: 신경망의 대표적인 문제로, 플라토 문제와 기울기 소멸 문제에 기인함
  - 활성화 함수의 변형: ReLU 함수와 이를 변형한 함수를 사용함
  - 가중치 초기화: 셀 포화가 일어나지 않도록 작은 값이면서도 각 뉴런의 가중치가 서로 달라지도록 랜덤한 값으로의 초기화가 중요함
  - 모멘텀: 기울기 수정식의 파라미터 변화량에 이전의 움직임(관성)을 반영하여 학습 속도의 저하를 방지하거나 학습의 불안정성을 감소시킴
    - NAG 기법 등
  - 적응적 학습률: 가중치마다 서로 다른 학습률을 갖고, 가중치가 변화된 크기의 누적합을 활용하여 변화폭에 따라 학습률을 적응적으로 결정하는 방법
    - RMSProp, AdaDelta, Adam 등
  - 배치 정규화: 학습하는 동안 각 노드의 활성화 함수로 들어가는 데이터 배치에 대한 입력이 셀 포화를 발생하지 않는 범위 내에 존재하도록 정규화시키는 방법
  - 2차 미분 방법: 기울기 수정식에서 기울기의 변화량을 결정할 때 오차함수의 2차 미분인 곡률 정보를 함께 사용하는 방법
- 과다적합 문제: 학습 데이터에 포함된 노이즈까지 학습하게 되어 새로운 테스트 데이터에 대하여 정확도가 떨어지는 현상
  - 조기 종료: 학습이 진행되는 과정에서 과다적합이 발생하기 전에 학습을 종료하는 방법
  - 정규항 추가: 학습하는 동안 가중치가 지나치게 커지는 것을 방지하기 위해 원래 오차함수에 가중치 벡터의 2차 노름에 해당하는 항을 추가하는 방법
  - 드롭아웃: 학습 과정에서 가중치를 수정할 때 임의로 선택한 은닉 노드의 일부를 제외하는 방법
  - 데이터 증대: 원래 데이터에 대해 인위적인 변형을 가하여 추가적인 데이터의 생성을 통해 많은 학습 데이터를 만들어 일반화 성능을 향상하는 방법

## 콘볼루션층에서 연산을 적용하는 방법 및 표현 방법의 이해(필터 크기, 패딩, 보폭, 다중채널)

- 합성곱 신경망(CNN): 인간의 시각 피질에 존재하는 신경세포들의 정보처리 기제로부터 영감을 받아서 영상 데이터처럼 격자 구조를 가진 데이터에 적합하도록 개발된 모델
  - 딥러닝 응용에서 가장 많이 사용되는 심층 신경망의 형태
- 신경세포: 입력층과 출력층을 제외하고 세 가지 유형의 세포들이 층을 구성함
  - 콘볼루션층: 콘볼루션 연산을 수행하여 특징맵을 형성함
  - 풀림층(서브샘플링층): 콘볼루션 연산의 결과에 대해 풀링 연산을 적용하여 특징맵을 다운샘플링함
  - 완전연결층: 기존 MLP에 해당하는 구조를 가지며, 앞선 층에서 추출된 특징 벡터를 입력받아 분류 작업을 수행함
- 콘볼루션층: 주어진 2D 입력에 대해 콘볼루션 연산을 반복적으로 수행하여 특징맵을 생성하는 층
  - 콘볼루션 연산: 해당하는 위치의 요소에 필터의 가중치를 곱해서 모두 더하는 간단한 선형 연산
  - 필터(커널, 마스크, 윈도): 3X3, 5X5, 7X7 등과 같은 크기를 가지며, 여기에 표현된 값이 CNN에서의 학습 대상이 되는 가중치
  - 다수의 필터를 사용해서 다수의 특징맵 생성이 가능함
  - 패딩: 입력 데이터의 가장자리를 0 값으로 채워 데이터의 가장자리에 대해서도 콘볼루션 연산을 동일하게 적용할 수 있도록 함
  - 보폭: 필터의 이동 간격 → 충력 특징맵의 크기 조정 가능
- 풀링층: 보통 콘볼루션층을 수행한 다음에 거치는 층
  - 풀링 연산으로 특징맵의 크기를 작게 만듦 → 계산 속도 향상 및 정보의 추상화
  - 연산종류: 최대 풀링, 평균 풀링, 가중치 평균 풀링 등
  - 풀링 연산의 특징: 연산 전후의 특징맵의 수는 변하지 않고, 데이터의 작은 이동에 대해서도 결과값이 변화되지 않음
  - 학습을 통해 결정될 파라미터가 없음. 즉, 풀링층에서는 학습이 수행되지 않음

## RNN의 개념과 응용 분야

- 순차 데이터: 음성, 문장, 동영상, 주식 시세와 같이 순서 정보를 가진 데이터
  - 특징: 출현 순서가 중요하고, 길이가 가변적이며, 데이터 안의 요소 사이에 문맥적인 의존성이 있음
- 순환 신경망: 시간에 따라 순차적으로 제공되는 정보를 다루기 위한 신경망
  - 입력층, 은닉층, 출력층으로 구성됨
  - 은닉 노드 사이에 가중치를 갖는 순환 에지가 존재하여 직전에 발생한 정보를 현재의 입력으로 전달함
  - 텍스트 처리와 음성 처리에서 주로 사용됨
- RNN 학습: 지도학습
  - 시간 역전파(BPTT) 학습 알고리즘 사용
  - 기울기 소멸 또는 기울기 폭발 문제 발생 → LSTM, GRU와 같은 셀이 등장
  - LSTM: 시간의 흐름과 무관하게 셀의 정보를 원하는 만큼 기억할 수 있도록 고안된 순환 신경망 셀
  - GRU: LSTM 셀 구조를 좀 더 단순하게 개선한 것

# 13장 딥러닝 응용

## 컴퓨터비전 분야의 대표적인 딥러닝 모델과 응용 분야

- 컴퓨터비전: 영상 데이터의 특성에 맞게 잘 설계된 특징을 추출하고, 이를 이용하여 추상적인 의미 정보를 파악하거나, 영상획득 기기의 특성을 반영하여 설계된 영상 개선 방법을 개발하는 등의 연구 분야
- 컴퓨터 비전 응용의 구분
  - 영상이해: 하나의 영상을 입력받아 그 안에 포함된 의미적인 정보를 분석하여 추상적인 개념이나 정량적인 정보량을 출력하는 문제
    - 다중 객체 검출 문제: 영상에 포함된 복수 개의 객체를 모두 검출하고 각 객체의 위치를 찾는 문제
      - R-CNN, Faster R-CNN, YOLO 모델
    - 영상설명 문제: 영상에 포함된 의미를 종합적으로 이해하고 이를 설명하는 자연어 문장을 생성하는 문제
      - Show and Tell 모델
  - 영상변화: 하나의 영상에 포함된 정보를 분석하여 원하는 형태로 변환된 새로운 영상을 출력하는 것
    - 오토인코더 모델: 입력이 주어졌을 때 출력도 입력과 같아지도록 학습함으로써 입력에 포함된 정보를 압축하였다가 다시 원래대로 복원할 수 있는 중간층의 축약된 특징을 얻는 것
      - 의료영상의 분할을 위해 개발된 U-net
  - 영상생성: 출력으로 새로운 영상을 생성하는 것
    - GAN 모델: 영상생성 모델에서 영상변환을 위한 모델로 확장
    - 생성기의 학습 목적은 최대한 진짜에 가까운 영상을 만들어 판별기를 속이는 것
  - 다양한 입력 형태로의 확장

## NLP를 위한 머신러닝 기법에서 데이터 표현 방법의 종류와 특징
- 자연어처리(NLP): 다양한 자연어를 이해하고, 번역하고, 다루는 전반적인 내용을 모두 포함하는 연구 분야
- 자연어처리를 위한 머신러닝 기법
  - 데이터수집
    - 말뭉치: 다양한 정보원으로부터 획득한 텍스트 데이터 집합
    - 종류: 구글 n-gram 말뭉치, 현대 미국영어 말뭉치(COCA), 세종말뭉치 등
  - 전처리
    - 토큰화: 말뭉치를 의미 있는 기본 단위인 토큰으로 구분하는 작업
    - 정제: 구두점이나 특수문자, 관사 등 데이터 분석에 방해가 되는 일종의 잡음을 제거하는 방법
    - 정규화: 표현 방법이 다른 단어들을 하나의 단어로 통일시키는 작업
    - 품사 태깅: 토큰화된 데이터의 품사를 결정하는 작업
  - 데이터 표현
    - 원핫인코딩: 각각의 토큰을 하나의 벡터로 표현하는 방식
    - BoW 방법: 말뭉치에 표함된 단어들의 빈도수에 기반하여 표현하는 방식
    - TF-IDF: 각 단어의 빈도수와 문서의 빈도를 함께 고려한 표현 방식
  - 데이터 분석: 워드 임베딩(단어들의 의미와 문맥 관계를 포함하여 단어를 벡터로 표현)과 이를 이용한 언어 모델링 수행
    - 워드 임베딩의 대표적인 기법으로 Word2Vec이 있으며, 학습 태스크에 따라 CBoW 모델과 Skip-gram 모델로 구분함

## 워드 임베딩

## 언어 모델의 종류와 특징

- 언어 모델: 단어의 시퀀스를 입력으로 받아 주어진 단어의 시퀀스가 실제로 사용되는 자연어 표현에서 관찰된 가능성을 확률값으로 출력하는 함수
- RNN 구조: LSTM 기반의 순환 신경망을 이용하여 한 번에 한 단어씩 차례로 입력받은 단어들을 먼저 원핫벡터로 표현하고, 이를 워드 임베딩에 적용하여 저차원의 실수 벡터를 얻고, 이것이 LSTM의 입력으로 전달됨
- Seq2Seq 모델: 문장을 입력받아 하나의 특징벡터로 변환하는 인코더 모듈과 특징벡터를 다시 다른 시퀀스로 변환하여 출력하는 디코더 모듈로 구성됨
- Attention 모델(주의집중 모델): 입력 문장을 하나의 특징벡터로 변환하는 대신, 디코더 부분에서 출력 단어를 생성할 때마다 인코더의 각 셀의 정보를 읽어오는 방식을 사용함
- 트랜스포머 모델: 복잡한 RNN 구조를 없애면서 잘 설계된 주의집중 모델만 활용함
- 최신 언어 모델
  - ELMo 모델: LSTM 기반의 Seq2Seq 구조의 모델
  - BERT 모델: 트랜스포머 기반의 모델로, 사전학습과 미세조정 전략 사용, 자연어처리 문제의 유형에 따른 활용 방식과 함께 학습된 모델 제공

# 14장 강화학습

- 강화학습: 에이전트가 현재 상태를 입력받아 적절한 행동을 결정하면, 현재 상태가 다른 상태로의 전이가 발생하고, 전이된 상태의 상대적 효과와 가치에 따라 보상이 결정됨
- 마르코프 결정 프로세스(MDP): 에이전트와 환경의 상호작용을 위해 문제를 정의할 때 주어지는 정보를 수학적으로 표현한 것
